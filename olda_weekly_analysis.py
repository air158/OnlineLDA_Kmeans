import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.sparse import lil_matrix
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics import silhouette_score
import warnings
import time
from datetime import datetime, timedelta
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

print("="*80)
print("åœ¨çº¿LDA (OLDA) æŒ‰å‘¨åˆ†æ®µä¸»é¢˜å»ºæ¨¡åˆ†æ")
print("="*80)

# ===========================
# 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
# ===========================
print("\n[æ­¥éª¤1] æ•°æ®åŠ è½½å’Œé¢„å¤„ç†")
print("-"*80)
start_time = time.time()

df = pd.read_csv('cn_bow.csv')
df['date'] = pd.to_datetime(df['date'])
print(f"âœ“ æ•°æ®è¯»å–å®Œæˆ")
print(f"  - æ€»æ–‡æ¡£æ•°: {len(df):,}")
print(f"  - æ—¥æœŸèŒƒå›´: {df['date'].min()} è‡³ {df['date'].max()}")

# è®¡ç®—æ€»å¤©æ•°å¹¶æŒ‰å‘¨åˆ†æ®µ
min_date = df['date'].min()
max_date = df['date'].max()
total_days = (max_date - min_date).days + 1
print(f"  - æ€»å¤©æ•°: {total_days}")

# åˆ›å»º4å‘¨çš„æ—¥æœŸèŒƒå›´
week_ranges = []
start_date = min_date
for i in range(4):
    end_date = start_date + timedelta(days=7)
    if end_date > max_date:
        end_date = max_date
    week_ranges.append((start_date, end_date, i+1))
    start_date = end_date

print(f"\n  æŒ‰å‘¨åˆ’åˆ†:")
for start, end, week_num in week_ranges:
    week_data = df[(df['date'] >= start) & (df['date'] < end)]
    print(f"    ç¬¬{week_num}å‘¨: {start.date()} è‡³ {end.date()} ({len(week_data)} æ–‡æ¡£)")

def parse_bow_vector(bow_string):
    """å°† 'è¯ID:é¢‘æ¬¡ è¯ID:é¢‘æ¬¡' æ ¼å¼è½¬æ¢ä¸ºå­—å…¸"""
    bow_dict = {}
    for item in bow_string.split():
        word_id, count = item.split(':')
        bow_dict[int(word_id)] = int(count)
    return bow_dict

# è§£æè¯è¢‹å‘é‡
all_word_ids = set()
bow_vectors = []
for bow_str in df['bow_vector']:
    bow_dict = parse_bow_vector(bow_str)
    bow_vectors.append(bow_dict)
    all_word_ids.update(bow_dict.keys())

# åˆ›å»ºè¯IDæ˜ å°„
word_id_to_idx = {word_id: idx for idx, word_id in enumerate(sorted(all_word_ids))}
idx_to_word_id = {idx: word_id for word_id, idx in word_id_to_idx.items()}
n_features = len(all_word_ids)

print(f"  - è¯æ±‡è¡¨å¤§å°: {n_features:,}")

# æ„å»ºç¨€ç–çŸ©é˜µ
X = lil_matrix((len(bow_vectors), n_features), dtype=np.float32)
for i, bow_dict in enumerate(bow_vectors):
    for word_id, count in bow_dict.items():
        X[i, word_id_to_idx[word_id]] = count

X = X.tocsr()
sparsity = 1 - X.nnz / (X.shape[0] * X.shape[1])
print(f"  - çŸ©é˜µå½¢çŠ¶: {X.shape}")
print(f"  - ç¨€ç–åº¦: {sparsity:.2%}")
print(f"âœ“ é¢„å¤„ç†å®Œæˆ (è€—æ—¶: {time.time()-start_time:.2f}ç§’)")

# ===========================
# 2. æŒ‰å‘¨è¿›è¡ŒOLDAä¸»é¢˜å»ºæ¨¡
# ===========================
print("\n[æ­¥éª¤2] æŒ‰å‘¨è¿›è¡ŒOLDAä¸»é¢˜å»ºæ¨¡")
print("="*80)

# å‚æ•°è®¾ç½®
n_topics_list = [3, 5, 8, 10]  # æµ‹è¯•ä¸åŒçš„ä¸»é¢˜æ•°
best_n_topics = 5  # é»˜è®¤æœ€ä½³ä¸»é¢˜æ•°

weekly_results = []

for start_date, end_date, week_num in week_ranges:
    print(f"\n{'='*80}")
    print(f"ç¬¬{week_num}å‘¨åˆ†æ: {start_date.date()} è‡³ {end_date.date()}")
    print(f"{'='*80}")

    # ç­›é€‰è¯¥å‘¨çš„æ•°æ®
    week_mask = (df['date'] >= start_date) & (df['date'] < end_date)
    week_indices = df[week_mask].index.tolist()
    X_week = X[week_indices]

    print(f"  æ–‡æ¡£æ•°: {len(week_indices)}")

    # æµ‹è¯•ä¸åŒä¸»é¢˜æ•°
    week_topic_results = []
    for n_topics in n_topics_list:
        print(f"\n  æµ‹è¯•ä¸»é¢˜æ•°: {n_topics}")
        start_time = time.time()

        # ä½¿ç”¨åœ¨çº¿LDA
        olda = LatentDirichletAllocation(
            n_components=n_topics,
            learning_method='online',
            batch_size=128,
            max_iter=50,
            learning_offset=50.,
            random_state=42,
            n_jobs=-1
        )

        doc_topic_dist = olda.fit_transform(X_week)
        fit_time = time.time() - start_time

        # è®¡ç®—å›°æƒ‘åº¦
        perplexity = olda.perplexity(X_week)

        # è®¡ç®—ä¸»é¢˜ä¸€è‡´æ€§ï¼ˆä½¿ç”¨è½®å»“ç³»æ•°ä½œä¸ºæ›¿ä»£æŒ‡æ ‡ï¼‰
        topic_labels = doc_topic_dist.argmax(axis=1)
        if len(set(topic_labels)) > 1:
            silhouette = silhouette_score(doc_topic_dist, topic_labels)
        else:
            silhouette = -1

        week_topic_results.append({
            'n_topics': n_topics,
            'perplexity': perplexity,
            'silhouette': silhouette,
            'time': fit_time,
            'model': olda,
            'doc_topic_dist': doc_topic_dist
        })

        print(f"    - å›°æƒ‘åº¦: {perplexity:.2f}")
        print(f"    - è½®å»“ç³»æ•°: {silhouette:.4f}")
        print(f"    - è®­ç»ƒæ—¶é—´: {fit_time:.2f}ç§’")

    # é€‰æ‹©æœ€ä½³ä¸»é¢˜æ•°ï¼ˆå›°æƒ‘åº¦æœ€ä½ï¼‰
    best_result = min(week_topic_results, key=lambda x: x['perplexity'])
    print(f"\n  âœ“ æœ€ä½³ä¸»é¢˜æ•°: {best_result['n_topics']} (å›°æƒ‘åº¦: {best_result['perplexity']:.2f})")

    # æå–ä¸»é¢˜è¯
    print(f"\n  ä¸»é¢˜å…³é”®è¯ (Top 10):")
    feature_names = [idx_to_word_id[i] for i in range(n_features)]
    n_top_words = 10

    topic_keywords = []
    for topic_idx, topic in enumerate(best_result['model'].components_):
        top_indices = topic.argsort()[-n_top_words:][::-1]
        top_words = [str(feature_names[i]) for i in top_indices]
        top_weights = [topic[i] for i in top_indices]

        print(f"    ä¸»é¢˜ {topic_idx}: {' '.join(top_words[:5])}")

        topic_keywords.append({
            'week': week_num,
            'topic': topic_idx,
            'keywords': ' '.join(top_words),
            'weights': ' '.join([f'{w:.4f}' for w in top_weights])
        })

    # ä¿å­˜è¯¥å‘¨ç»“æœ
    weekly_results.append({
        'week_num': week_num,
        'start_date': start_date,
        'end_date': end_date,
        'n_docs': len(week_indices),
        'best_n_topics': best_result['n_topics'],
        'perplexity': best_result['perplexity'],
        'silhouette': best_result['silhouette'],
        'time': best_result['time'],
        'model': best_result['model'],
        'doc_topic_dist': best_result['doc_topic_dist'],
        'topic_keywords': topic_keywords,
        'all_results': week_topic_results,
        'week_indices': week_indices
    })

# ===========================
# 3. ç»“æœæ±‡æ€»å’Œå¯è§†åŒ–
# ===========================
print("\n" + "="*80)
print("[æ­¥éª¤3] ç»“æœæ±‡æ€»å’Œå¯è§†åŒ–")
print("="*80)

# åˆ›å»ºç»¼åˆå¯¹æ¯”å›¾
fig = plt.figure(figsize=(20, 12))

# 1. æ¯å‘¨æœ€ä½³å›°æƒ‘åº¦å¯¹æ¯”
ax1 = plt.subplot(2, 3, 1)
weeks = [r['week_num'] for r in weekly_results]
perplexities = [r['perplexity'] for r in weekly_results]
ax1.bar(weeks, perplexities, color='steelblue', edgecolor='black', linewidth=2)
ax1.set_xlabel('å‘¨æ¬¡', fontweight='bold', fontsize=12)
ax1.set_ylabel('å›°æƒ‘åº¦', fontweight='bold', fontsize=12)
ax1.set_title('å„å‘¨OLDAå›°æƒ‘åº¦å¯¹æ¯”', fontweight='bold', fontsize=14)
ax1.set_xticks(weeks)
ax1.set_xticklabels([f'ç¬¬{w}å‘¨' for w in weeks])
ax1.grid(True, alpha=0.3, axis='y')
for i, (w, p) in enumerate(zip(weeks, perplexities)):
    ax1.text(w, p + max(perplexities)*0.02, f'{p:.1f}', ha='center', va='bottom', fontweight='bold')

# 2. æ¯å‘¨æ–‡æ¡£æ•°é‡
ax2 = plt.subplot(2, 3, 2)
doc_counts = [r['n_docs'] for r in weekly_results]
ax2.bar(weeks, doc_counts, color='lightcoral', edgecolor='black', linewidth=2)
ax2.set_xlabel('å‘¨æ¬¡', fontweight='bold', fontsize=12)
ax2.set_ylabel('æ–‡æ¡£æ•°é‡', fontweight='bold', fontsize=12)
ax2.set_title('å„å‘¨æ–‡æ¡£æ•°é‡åˆ†å¸ƒ', fontweight='bold', fontsize=14)
ax2.set_xticks(weeks)
ax2.set_xticklabels([f'ç¬¬{w}å‘¨' for w in weeks])
ax2.grid(True, alpha=0.3, axis='y')
for i, (w, d) in enumerate(zip(weeks, doc_counts)):
    ax2.text(w, d + max(doc_counts)*0.02, f'{d}', ha='center', va='bottom', fontweight='bold')

# 3. æ¯å‘¨æœ€ä½³ä¸»é¢˜æ•°
ax3 = plt.subplot(2, 3, 3)
best_topics = [r['best_n_topics'] for r in weekly_results]
ax3.bar(weeks, best_topics, color='lightgreen', edgecolor='black', linewidth=2)
ax3.set_xlabel('å‘¨æ¬¡', fontweight='bold', fontsize=12)
ax3.set_ylabel('æœ€ä½³ä¸»é¢˜æ•°', fontweight='bold', fontsize=12)
ax3.set_title('å„å‘¨æœ€ä½³ä¸»é¢˜æ•°', fontweight='bold', fontsize=14)
ax3.set_xticks(weeks)
ax3.set_xticklabels([f'ç¬¬{w}å‘¨' for w in weeks])
ax3.set_yticks(range(min(best_topics), max(best_topics)+1))
ax3.grid(True, alpha=0.3, axis='y')
for i, (w, t) in enumerate(zip(weeks, best_topics)):
    ax3.text(w, t + 0.1, f'{t}', ha='center', va='bottom', fontweight='bold')

# 4. è®­ç»ƒæ—¶é—´å¯¹æ¯”
ax4 = plt.subplot(2, 3, 4)
times = [r['time'] for r in weekly_results]
ax4.bar(weeks, times, color='gold', edgecolor='black', linewidth=2)
ax4.set_xlabel('å‘¨æ¬¡', fontweight='bold', fontsize=12)
ax4.set_ylabel('è®­ç»ƒæ—¶é—´ (ç§’)', fontweight='bold', fontsize=12)
ax4.set_title('å„å‘¨OLDAè®­ç»ƒæ—¶é—´', fontweight='bold', fontsize=14)
ax4.set_xticks(weeks)
ax4.set_xticklabels([f'ç¬¬{w}å‘¨' for w in weeks])
ax4.grid(True, alpha=0.3, axis='y')
for i, (w, t) in enumerate(zip(weeks, times)):
    ax4.text(w, t + max(times)*0.02, f'{t:.1f}s', ha='center', va='bottom', fontweight='bold')

# 5. å›°æƒ‘åº¦éšä¸»é¢˜æ•°å˜åŒ–ï¼ˆé€‰å–ç¬¬1å‘¨ä½œä¸ºç¤ºä¾‹ï¼‰
ax5 = plt.subplot(2, 3, 5)
week1_results = weekly_results[0]['all_results']
n_topics_tested = [r['n_topics'] for r in week1_results]
perplexities_tested = [r['perplexity'] for r in week1_results]
ax5.plot(n_topics_tested, perplexities_tested, 'o-', linewidth=2, markersize=8, color='purple')
ax5.scatter([weekly_results[0]['best_n_topics']], [weekly_results[0]['perplexity']],
           color='red', s=200, marker='*', zorder=5, label='æœ€ä½³')
ax5.set_xlabel('ä¸»é¢˜æ•°', fontweight='bold', fontsize=12)
ax5.set_ylabel('å›°æƒ‘åº¦', fontweight='bold', fontsize=12)
ax5.set_title('ç¬¬1å‘¨: ä¸»é¢˜æ•° vs å›°æƒ‘åº¦', fontweight='bold', fontsize=14)
ax5.grid(True, alpha=0.3)
ax5.legend()

# 6. ä¸»é¢˜åˆ†å¸ƒçƒ­åŠ›å›¾ï¼ˆç¬¬1å‘¨ï¼‰
ax6 = plt.subplot(2, 3, 6)
doc_topic = weekly_results[0]['doc_topic_dist']
# åªæ˜¾ç¤ºå‰50ä¸ªæ–‡æ¡£çš„ä¸»é¢˜åˆ†å¸ƒ
if len(doc_topic) > 50:
    doc_topic_display = doc_topic[:50]
else:
    doc_topic_display = doc_topic
im = ax6.imshow(doc_topic_display.T, aspect='auto', cmap='YlOrRd', interpolation='nearest')
ax6.set_xlabel('æ–‡æ¡£ç´¢å¼•', fontweight='bold', fontsize=12)
ax6.set_ylabel('ä¸»é¢˜', fontweight='bold', fontsize=12)
ax6.set_title(f'ç¬¬1å‘¨: æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒçƒ­åŠ›å›¾ (å‰{len(doc_topic_display)}æ–‡æ¡£)', fontweight='bold', fontsize=14)
plt.colorbar(im, ax=ax6, label='ä¸»é¢˜æ¦‚ç‡')

plt.tight_layout()
plt.savefig('olda_weekly_analysis.png', dpi=300, bbox_inches='tight')
print("âœ“ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: olda_weekly_analysis.png")

# ===========================
# 4. ä¿å­˜ç»“æœ
# ===========================
print("\n[æ­¥éª¤4] ä¿å­˜åˆ†æç»“æœ")
print("-"*80)

# ä¿å­˜æ¯å‘¨ä¸»é¢˜å…³é”®è¯
all_keywords = []
for result in weekly_results:
    all_keywords.extend(result['topic_keywords'])
keywords_df = pd.DataFrame(all_keywords)
keywords_df.to_csv('olda_weekly_topic_keywords.csv', index=False, encoding='utf-8-sig')
print("âœ“ æ¯å‘¨ä¸»é¢˜å…³é”®è¯å·²ä¿å­˜: olda_weekly_topic_keywords.csv")

# ä¿å­˜æ¯å‘¨æ±‡æ€»ç»“æœ
summary_data = []
for result in weekly_results:
    summary_data.append({
        'å‘¨æ¬¡': f'ç¬¬{result["week_num"]}å‘¨',
        'å¼€å§‹æ—¥æœŸ': result['start_date'].date(),
        'ç»“æŸæ—¥æœŸ': result['end_date'].date(),
        'æ–‡æ¡£æ•°': result['n_docs'],
        'æœ€ä½³ä¸»é¢˜æ•°': result['best_n_topics'],
        'å›°æƒ‘åº¦': result['perplexity'],
        'è½®å»“ç³»æ•°': result['silhouette'],
        'è®­ç»ƒæ—¶é—´(ç§’)': result['time']
    })
summary_df = pd.DataFrame(summary_data)
summary_df.to_csv('olda_weekly_summary.csv', index=False, encoding='utf-8-sig')
print("âœ“ æ¯å‘¨æ±‡æ€»ç»“æœå·²ä¿å­˜: olda_weekly_summary.csv")

# ä¿å­˜æ–‡æ¡£çš„ä¸»é¢˜åˆ†é…
doc_topics_data = []
for result in weekly_results:
    week_num = result['week_num']
    doc_topic_dist = result['doc_topic_dist']
    week_indices = result['week_indices']

    for local_idx, global_idx in enumerate(week_indices):
        topic_probs = doc_topic_dist[local_idx]
        dominant_topic = topic_probs.argmax()
        doc_topics_data.append({
            'å‘¨æ¬¡': week_num,
            'æ–‡æ¡£ç´¢å¼•': global_idx,
            'æ—¥æœŸ': df.loc[global_idx, 'date'],
            'æ ‡ç­¾': df.loc[global_idx, 'label'],
            'ä¸»è¦ä¸»é¢˜': dominant_topic,
            'ä¸»é¢˜æ¦‚ç‡': topic_probs[dominant_topic],
            **{f'ä¸»é¢˜{i}æ¦‚ç‡': topic_probs[i] for i in range(len(topic_probs))}
        })

doc_topics_df = pd.DataFrame(doc_topics_data)
doc_topics_df.to_csv('olda_weekly_document_topics.csv', index=False, encoding='utf-8-sig')
print("âœ“ æ–‡æ¡£ä¸»é¢˜åˆ†é…å·²ä¿å­˜: olda_weekly_document_topics.csv")

# ç”Ÿæˆåˆ†ææŠ¥å‘Š
report = f"""
åœ¨çº¿LDA (OLDA) æŒ‰å‘¨åˆ†æ®µä¸»é¢˜å»ºæ¨¡åˆ†ææŠ¥å‘Š
{"="*80}

1. æ•°æ®æ¦‚å†µ
   - æ€»æ–‡æ¡£æ•°: {len(df):,}
   - æ—¥æœŸèŒƒå›´: {df['date'].min()} è‡³ {df['date'].max()}
   - æ€»å¤©æ•°: {total_days}
   - è¯æ±‡è¡¨å¤§å°: {n_features:,}
   - çŸ©é˜µç¨€ç–åº¦: {sparsity:.2%}

2. åˆ†å‘¨åˆ†æç»“æœ
"""

for result in weekly_results:
    report += f"""
   ç¬¬{result['week_num']}å‘¨ ({result['start_date'].date()} è‡³ {result['end_date'].date()})
   -----------------------------------------
   - æ–‡æ¡£æ•°: {result['n_docs']:,}
   - æœ€ä½³ä¸»é¢˜æ•°: {result['best_n_topics']}
   - å›°æƒ‘åº¦: {result['perplexity']:.2f}
   - è½®å»“ç³»æ•°: {result['silhouette']:.4f}
   - è®­ç»ƒæ—¶é—´: {result['time']:.2f}ç§’

   ä¸»é¢˜å…³é”®è¯:
"""
    for topic_kw in result['topic_keywords']:
        keywords = ' '.join(topic_kw['keywords'].split()[:5])
        report += f"      ä¸»é¢˜{topic_kw['topic']}: {keywords}\n"

report += f"""

3. æ•´ä½“è¶‹åŠ¿åˆ†æ
   - å¹³å‡å›°æƒ‘åº¦: {np.mean(perplexities):.2f}
   - å›°æƒ‘åº¦æ ‡å‡†å·®: {np.std(perplexities):.2f}
   - å¹³å‡ä¸»é¢˜æ•°: {np.mean(best_topics):.1f}
   - æ€»è®­ç»ƒæ—¶é—´: {sum(times):.2f}ç§’

4. ä¸»è¦å‘ç°
   - å›°æƒ‘åº¦æœ€ä½çš„å‘¨: ç¬¬{perplexities.index(min(perplexities)) + 1}å‘¨ (å›°æƒ‘åº¦: {min(perplexities):.2f})
   - æ–‡æ¡£æ•°æœ€å¤šçš„å‘¨: ç¬¬{doc_counts.index(max(doc_counts)) + 1}å‘¨ ({max(doc_counts)} æ–‡æ¡£)
   - ä¸»é¢˜æ•°æœ€å¤šçš„å‘¨: ç¬¬{best_topics.index(max(best_topics)) + 1}å‘¨ ({max(best_topics)} ä¸»é¢˜)

5. å»ºè®®
   - OLDAæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å„å‘¨æ•°æ®çš„ä¸»é¢˜å»ºæ¨¡
   - ä¸åŒå‘¨çš„ä¸»é¢˜æ•°å­˜åœ¨å·®å¼‚ï¼Œåæ˜ äº†å†…å®¹çš„å¤šæ ·æ€§å˜åŒ–
   - å»ºè®®æ ¹æ®å…·ä½“å‘¨çš„ä¸»é¢˜å…³é”®è¯è¿›è¡Œæ·±å…¥çš„å†…å®¹åˆ†æ

6. è¾“å‡ºæ–‡ä»¶
   - olda_weekly_analysis.png: å„å‘¨OLDAåˆ†æå¯è§†åŒ–
   - olda_weekly_topic_keywords.csv: æ¯å‘¨ä¸»é¢˜å…³é”®è¯
   - olda_weekly_summary.csv: æ¯å‘¨æ±‡æ€»ç»Ÿè®¡
   - olda_weekly_document_topics.csv: æ–‡æ¡£çº§ä¸»é¢˜åˆ†é…
   - olda_weekly_report.txt: æœ¬åˆ†ææŠ¥å‘Š
"""

with open('olda_weekly_report.txt', 'w', encoding='utf-8') as f:
    f.write(report)
print("âœ“ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: olda_weekly_report.txt")

print("\n" + "="*80)
print("âœ… OLDAæŒ‰å‘¨åˆ†æ®µåˆ†æå®Œæˆï¼")
print("="*80)
print(f"\nğŸ“ ç”Ÿæˆæ–‡ä»¶:")
print(f"   1. olda_weekly_analysis.png - å„å‘¨åˆ†æå¯è§†åŒ–")
print(f"   2. olda_weekly_topic_keywords.csv - æ¯å‘¨ä¸»é¢˜å…³é”®è¯")
print(f"   3. olda_weekly_summary.csv - æ¯å‘¨æ±‡æ€»ç»Ÿè®¡")
print(f"   4. olda_weekly_document_topics.csv - æ–‡æ¡£ä¸»é¢˜åˆ†é…")
print(f"   5. olda_weekly_report.txt - è¯¦ç»†åˆ†ææŠ¥å‘Š")
print("="*80)

# æ‰“å°ç®€è¦æ±‡æ€»
print("\nã€å„å‘¨ç®€è¦æ€»ç»“ã€‘")
for result in weekly_results:
    print(f"  ç¬¬{result['week_num']}å‘¨: {result['n_docs']:4d}æ–‡æ¡£, {result['best_n_topics']}ä¸»é¢˜, å›°æƒ‘åº¦{result['perplexity']:6.1f}")

plt.show()
