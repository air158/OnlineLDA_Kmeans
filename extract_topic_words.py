import pandas as pd
import numpy as np
from scipy.sparse import lil_matrix
from sklearn.decomposition import LatentDirichletAllocation
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("LDAä¸»é¢˜è¯æå–")
print("="*80)

# ===========================
# 1. åŠ è½½æ•°æ®å’Œè¯å…¸
# ===========================
print("\n[æ­¥éª¤1] åŠ è½½æ•°æ®å’Œè¯å…¸")
print("-"*80)

# è¯»å–è¯è¢‹å‘é‡æ•°æ®
df = pd.read_csv('cn_bow.csv')

# è¯»å–è¯å…¸ï¼ˆè¯IDåˆ°è¯çš„æ˜ å°„ï¼‰
print("æ­£åœ¨åŠ è½½è¯å…¸...")
word_dict = {}
try:
    with open('cn_dictionary.txt', 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            # æ ¼å¼: è¯ è¯IDï¼ˆç©ºæ ¼åˆ†éš”ï¼‰
            parts = line.rsplit(' ', 1)  # ä»å³è¾¹åˆ†å‰²ä¸€æ¬¡ï¼Œé˜²æ­¢è¯ä¸­åŒ…å«ç©ºæ ¼
            if len(parts) == 2:
                word = parts[0]
                word_id = int(parts[1])
                word_dict[word_id] = word
    print(f"âœ“ è¯å…¸åŠ è½½å®Œæˆï¼Œå…± {len(word_dict):,} ä¸ªè¯")
except FileNotFoundError:
    print("âš ï¸  æœªæ‰¾åˆ°è¯å…¸æ–‡ä»¶ cn_dictionary.txt")
    print("   å°†ä½¿ç”¨è¯IDä»£æ›¿å®é™…è¯æ±‡")
    word_dict = None
except Exception as e:
    print(f"âš ï¸  åŠ è½½è¯å…¸æ—¶å‡ºé”™: {e}")
    print("   å°†ä½¿ç”¨è¯IDä»£æ›¿å®é™…è¯æ±‡")
    word_dict = None

# ===========================
# 2. æ„å»ºæ–‡æ¡£-è¯çŸ©é˜µ
# ===========================
print("\n[æ­¥éª¤2] æ„å»ºæ–‡æ¡£-è¯çŸ©é˜µ")
print("-"*80)

def parse_bow_vector(bow_string):
    """å°† 'è¯ID:é¢‘æ¬¡ è¯ID:é¢‘æ¬¡' æ ¼å¼è½¬æ¢ä¸ºå­—å…¸"""
    bow_dict = {}
    for item in bow_string.split():
        word_id, count = item.split(':')
        bow_dict[int(word_id)] = int(count)
    return bow_dict

# è§£æè¯è¢‹å‘é‡
all_word_ids = set()
bow_vectors = []
for bow_str in df['bow_vector']:
    bow_dict = parse_bow_vector(bow_str)
    bow_vectors.append(bow_dict)
    all_word_ids.update(bow_dict.keys())

# åˆ›å»ºè¯IDåˆ°åˆ—ç´¢å¼•çš„æ˜ å°„
word_id_to_idx = {word_id: idx for idx, word_id in enumerate(sorted(all_word_ids))}
idx_to_word_id = {idx: word_id for word_id, idx in word_id_to_idx.items()}
n_features = len(all_word_ids)

print(f"æ–‡æ¡£æ•°é‡: {len(bow_vectors):,}")
print(f"è¯æ±‡è¡¨å¤§å°: {n_features:,}")

# æ„å»ºç¨€ç–çŸ©é˜µ
print("æ­£åœ¨æ„å»ºç¨€ç–çŸ©é˜µ...")
X = lil_matrix((len(bow_vectors), n_features), dtype=np.float32)
for i, bow_dict in enumerate(bow_vectors):
    for word_id, count in bow_dict.items():
        X[i, word_id_to_idx[word_id]] = count

X = X.tocsr()
print(f"âœ“ çŸ©é˜µæ„å»ºå®Œæˆ: {X.shape}")
print(f"  ç¨€ç–åº¦: {1 - X.nnz / (X.shape[0] * X.shape[1]):.2%}")

# ===========================
# 3. è®­ç»ƒLDAæ¨¡å‹
# ===========================
print("\n[æ­¥éª¤3] è®­ç»ƒLDAæ¨¡å‹ï¼ˆæœ€ä¼˜é…ç½®ï¼š4ä¸ªä¸»é¢˜ï¼‰")
print("-"*80)

n_topics = 4  # æ ¹æ®å®éªŒä¸€çš„ç»“æœï¼Œæœ€ä¼˜ä¸»é¢˜æ•°ä¸º4
n_top_words = 20  # æ¯ä¸ªä¸»é¢˜æå–çš„å…³é”®è¯æ•°é‡

print(f"è®­ç»ƒå‚æ•°ï¼š")
print(f"  - ä¸»é¢˜æ•°é‡: {n_topics}")
print(f"  - æ¯ä¸ªä¸»é¢˜æå–: å‰{n_top_words}ä¸ªå…³é”®è¯")

lda = LatentDirichletAllocation(
    n_components=n_topics,
    max_iter=50,  # å¢åŠ è¿­ä»£æ¬¡æ•°ä»¥æé«˜è´¨é‡
    learning_method='online',
    learning_decay=0.7,
    learning_offset=50.0,
    batch_size=512,
    random_state=42,
    n_jobs=1,
    verbose=0
)

print("æ­£åœ¨è®­ç»ƒLDAæ¨¡å‹...")
lda.fit(X)
perplexity = lda.perplexity(X)

print(f"âœ“ è®­ç»ƒå®Œæˆï¼")
print(f"  å›°æƒ‘åº¦: {perplexity:.2f}")

# ===========================
# 4. æå–ä¸»é¢˜è¯
# ===========================
print("\n[æ­¥éª¤4] æå–å„ä¸»é¢˜çš„å…³é”®è¯")
print("="*80)

def get_top_words(model, feature_names, n_top_words=20):
    """æå–æ¯ä¸ªä¸»é¢˜çš„topè¯"""
    topics = []
    for topic_idx, topic in enumerate(model.components_):
        top_indices = topic.argsort()[-n_top_words:][::-1]
        top_words = []
        for idx in top_indices:
            word_id = idx_to_word_id[idx]
            word = word_dict.get(word_id, f"è¯ID_{word_id}") if word_dict else f"è¯ID_{word_id}"
            weight = topic[idx]
            top_words.append((word, weight, word_id))
        topics.append(top_words)
    return topics

# æå–ä¸»é¢˜è¯
topics = get_top_words(lda, word_id_to_idx, n_top_words)

# ===========================
# 5. å±•ç¤ºå’Œä¿å­˜ç»“æœ
# ===========================
print("\n" + "="*80)
print("ä¸»é¢˜è¯æå–ç»“æœ")
print("="*80)

# å‡†å¤‡ä¿å­˜çš„æ•°æ®
all_topics_data = []

for topic_idx, topic_words in enumerate(topics):
    print(f"\nã€ä¸»é¢˜ {topic_idx + 1}ã€‘")
    print("-"*80)

    # æ˜¾ç¤ºå‰20ä¸ªè¯
    print(f"{'æ’å':<6} {'è¯æ±‡':<20} {'æƒé‡':<12} {'è¯ID':<10}")
    print("-"*80)

    for rank, (word, weight, word_id) in enumerate(topic_words, 1):
        print(f"{rank:<6} {word:<20} {weight:<12.6f} {word_id:<10}")

        # ä¿å­˜æ•°æ®
        all_topics_data.append({
            'ä¸»é¢˜ç¼–å·': topic_idx + 1,
            'æ’å': rank,
            'è¯æ±‡': word,
            'æƒé‡': weight,
            'è¯ID': word_id
        })

    # æ˜¾ç¤ºå‰10ä¸ªè¯çš„ç®€è¦åˆ—è¡¨
    top10_words = [word for word, _, _ in topic_words[:10]]
    print(f"\n  ğŸ”‘ Top 10å…³é”®è¯: {', '.join(top10_words)}")

# ä¿å­˜åˆ°CSV
topics_df = pd.DataFrame(all_topics_data)
topics_df.to_csv('lda_topic_words.csv', index=False, encoding='utf-8-sig')
print("\n" + "="*80)
print("âœ“ ä¸»é¢˜è¯å·²ä¿å­˜åˆ°: lda_topic_words.csv")

# ===========================
# 6. æ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒåˆ†æ
# ===========================
print("\n[æ­¥éª¤5] åˆ†ææ–‡æ¡£-ä¸»é¢˜åˆ†å¸ƒ")
print("-"*80)

# è·å–æ–‡æ¡£çš„ä¸»é¢˜åˆ†å¸ƒ
doc_topic_dist = lda.transform(X)

# ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ†é…ä¸»è¦ä¸»é¢˜
dominant_topics = np.argmax(doc_topic_dist, axis=1)

# ç»Ÿè®¡æ¯ä¸ªä¸»é¢˜çš„æ–‡æ¡£æ•°é‡
topic_counts = np.bincount(dominant_topics, minlength=n_topics)

print("\nå„ä¸»é¢˜çš„æ–‡æ¡£åˆ†å¸ƒ:")
print("-"*80)
for topic_idx in range(n_topics):
    count = topic_counts[topic_idx]
    percentage = count / len(dominant_topics) * 100
    print(f"ä¸»é¢˜ {topic_idx + 1}: {count:>5} ç¯‡æ–‡æ¡£ ({percentage:>5.1f}%)")

# ä¸ºåŸå§‹æ•°æ®æ·»åŠ ä¸»é¢˜æ ‡ç­¾
df['ä¸»è¦ä¸»é¢˜'] = dominant_topics + 1  # ä»1å¼€å§‹ç¼–å·
df['ä¸»é¢˜1_æ¦‚ç‡'] = doc_topic_dist[:, 0]
df['ä¸»é¢˜2_æ¦‚ç‡'] = doc_topic_dist[:, 1]
df['ä¸»é¢˜3_æ¦‚ç‡'] = doc_topic_dist[:, 2]
df['ä¸»é¢˜4_æ¦‚ç‡'] = doc_topic_dist[:, 3]

# ä¿å­˜å¸¦ä¸»é¢˜æ ‡ç­¾çš„æ•°æ®
df.to_csv('documents_with_topics.csv', index=False, encoding='utf-8-sig')
print("\nâœ“ å¸¦ä¸»é¢˜æ ‡ç­¾çš„æ–‡æ¡£å·²ä¿å­˜åˆ°: documents_with_topics.csv")

# ===========================
# 7. æå–æ¯ä¸ªä¸»é¢˜çš„ä»£è¡¨æ€§æ–‡æ¡£
# ===========================
print("\n[æ­¥éª¤6] æå–å„ä¸»é¢˜çš„ä»£è¡¨æ€§æ–‡æ¡£")
print("-"*80)

representative_docs = []

for topic_idx in range(n_topics):
    print(f"\nã€ä¸»é¢˜ {topic_idx + 1} çš„ä»£è¡¨æ€§æ–‡æ¡£ã€‘")
    print("-"*80)

    # æ‰¾å‡ºè¯¥ä¸»é¢˜æ¦‚ç‡æœ€é«˜çš„3ä¸ªæ–‡æ¡£
    topic_probs = doc_topic_dist[:, topic_idx]
    top_doc_indices = topic_probs.argsort()[-3:][::-1]

    for rank, doc_idx in enumerate(top_doc_indices, 1):
        doc_date = df.iloc[doc_idx]['date']
        doc_label = df.iloc[doc_idx]['label']
        doc_prob = topic_probs[doc_idx]

        print(f"\n  æ–‡æ¡£ {rank}:")
        print(f"    æ—¥æœŸ: {doc_date}")
        print(f"    æ ‡ç­¾: {doc_label}")
        print(f"    ä¸»é¢˜æ¦‚ç‡: {doc_prob:.4f}")

        representative_docs.append({
            'ä¸»é¢˜ç¼–å·': topic_idx + 1,
            'æ’å': rank,
            'æ–‡æ¡£ç´¢å¼•': doc_idx,
            'æ—¥æœŸ': doc_date,
            'æ ‡ç­¾': doc_label,
            'ä¸»é¢˜æ¦‚ç‡': doc_prob
        })

# ä¿å­˜ä»£è¡¨æ€§æ–‡æ¡£
rep_docs_df = pd.DataFrame(representative_docs)
rep_docs_df.to_csv('representative_documents.csv', index=False, encoding='utf-8-sig')
print("\n" + "="*80)
print("âœ“ ä»£è¡¨æ€§æ–‡æ¡£å·²ä¿å­˜åˆ°: representative_documents.csv")

# ===========================
# 8. ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
# ===========================
print("\n[æ­¥éª¤7] ç”Ÿæˆä¸»é¢˜åˆ†ææŠ¥å‘Š")
print("-"*80)

report = f"""
LDAä¸»é¢˜æ¨¡å‹åˆ†ææŠ¥å‘Š
{"="*80}

1. æ¨¡å‹é…ç½®
   - ä¸»é¢˜æ•°é‡: {n_topics}
   - æ–‡æ¡£æ•°é‡: {len(bow_vectors):,}
   - è¯æ±‡è¡¨å¤§å°: {n_features:,}
   - å›°æƒ‘åº¦: {perplexity:.2f}

2. å„ä¸»é¢˜å…³é”®è¯ï¼ˆTop 10ï¼‰
{"="*80}

"""

for topic_idx, topic_words in enumerate(topics):
    report += f"\nä¸»é¢˜ {topic_idx + 1}:\n"
    top10_words = [word for word, _, _ in topic_words[:10]]
    report += f"  å…³é”®è¯: {', '.join(top10_words)}\n"
    report += f"  æ–‡æ¡£æ•°: {topic_counts[topic_idx]} ç¯‡ ({topic_counts[topic_idx]/len(dominant_topics)*100:.1f}%)\n"

report += f"""
{"="*80}

3. æ–‡æ¡£åˆ†å¸ƒç»Ÿè®¡
   - ä¸»é¢˜1: {topic_counts[0]:>5} ç¯‡ ({topic_counts[0]/len(dominant_topics)*100:>5.1f}%)
   - ä¸»é¢˜2: {topic_counts[1]:>5} ç¯‡ ({topic_counts[1]/len(dominant_topics)*100:>5.1f}%)
   - ä¸»é¢˜3: {topic_counts[2]:>5} ç¯‡ ({topic_counts[2]/len(dominant_topics)*100:>5.1f}%)
   - ä¸»é¢˜4: {topic_counts[3]:>5} ç¯‡ ({topic_counts[3]/len(dominant_topics)*100:>5.1f}%)

4. ç”Ÿæˆæ–‡ä»¶
   âœ“ lda_topic_words.csv - æ‰€æœ‰ä¸»é¢˜çš„å…³é”®è¯åˆ—è¡¨
   âœ“ documents_with_topics.csv - å¸¦ä¸»é¢˜æ ‡ç­¾çš„å®Œæ•´æ–‡æ¡£
   âœ“ representative_documents.csv - å„ä¸»é¢˜çš„ä»£è¡¨æ€§æ–‡æ¡£
   âœ“ lda_topic_analysis_report.txt - æœ¬åˆ†ææŠ¥å‘Š

{"="*80}
"""

with open('lda_topic_analysis_report.txt', 'w', encoding='utf-8') as f:
    f.write(report)

print("âœ“ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: lda_topic_analysis_report.txt")

# ===========================
# 9. ä¸»é¢˜-ä¸»é¢˜ç›¸ä¼¼åº¦åˆ†æ
# ===========================
print("\n[æ­¥éª¤8] ä¸»é¢˜é—´ç›¸ä¼¼åº¦åˆ†æ")
print("-"*80)

from sklearn.metrics.pairwise import cosine_similarity

# è®¡ç®—ä¸»é¢˜é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
topic_vectors = lda.components_
similarity_matrix = cosine_similarity(topic_vectors)

print("\nä¸»é¢˜é—´ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ:")
print("-"*80)
print(f"{'':>10}", end='')
for i in range(n_topics):
    print(f"ä¸»é¢˜{i+1:>6}", end='')
print()
print("-"*80)

for i in range(n_topics):
    print(f"ä¸»é¢˜{i+1:>6}   ", end='')
    for j in range(n_topics):
        print(f"{similarity_matrix[i][j]:>8.4f}", end='')
    print()

# æ‰¾å‡ºæœ€ç›¸ä¼¼å’Œæœ€ä¸ç›¸ä¼¼çš„ä¸»é¢˜å¯¹
similarities = []
for i in range(n_topics):
    for j in range(i+1, n_topics):
        similarities.append((i+1, j+1, similarity_matrix[i][j]))

similarities.sort(key=lambda x: x[2], reverse=True)

print("\nä¸»é¢˜å¯¹ç›¸ä¼¼åº¦æ’åº:")
print("-"*80)
for topic1, topic2, sim in similarities:
    print(f"ä¸»é¢˜{topic1} â†” ä¸»é¢˜{topic2}: {sim:.4f}")

# ===========================
# 10. æ€»ç»“
# ===========================
print("\n" + "="*80)
print("âœ… ä¸»é¢˜è¯æå–å®Œæˆï¼")
print("="*80)

print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
print("  1. lda_topic_words.csv - å„ä¸»é¢˜çš„Top 20å…³é”®è¯")
print("  2. documents_with_topics.csv - å¸¦ä¸»é¢˜æ ‡ç­¾çš„å®Œæ•´æ•°æ®")
print("  3. representative_documents.csv - å„ä¸»é¢˜çš„ä»£è¡¨æ€§æ–‡æ¡£")
print("  4. lda_topic_analysis_report.txt - ä¸»é¢˜åˆ†ææŠ¥å‘Š")

print("\nğŸ“Š ä¸»é¢˜åˆ†å¸ƒ:")
for topic_idx in range(n_topics):
    count = topic_counts[topic_idx]
    percentage = count / len(dominant_topics) * 100
    bar_length = int(percentage / 2)
    bar = 'â–ˆ' * bar_length
    print(f"  ä¸»é¢˜{topic_idx+1}: {bar} {count}ç¯‡ ({percentage:.1f}%)")

print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
print("  - æŸ¥çœ‹ lda_topic_words.csv äº†è§£å„ä¸»é¢˜çš„å…³é”®è¯")
print("  - æŸ¥çœ‹ representative_documents.csv äº†è§£å„ä¸»é¢˜çš„å…¸å‹æ–‡æ¡£")
print("  - ä½¿ç”¨ documents_with_topics.csv è¿›è¡Œè¿›ä¸€æ­¥åˆ†æ")
print("  - æ ¹æ®ä¸»é¢˜å…³é”®è¯ä¸ºå„ä¸»é¢˜å‘½å")

print("\n" + "="*80)
